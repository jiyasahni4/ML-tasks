{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mc1kKlF1uk-a",
        "outputId": "38d52a6c-1d38-4924-d9d1-af7036cc4e47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-Entropy: 1.0819778284410282\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def cross_entropy(a, b):\n",
        "    return -np.sum(a * np.log(b))\n",
        "\n",
        "# Example usiing 2 probability distributions\n",
        "a = np.array([0.2, 0.3, 0.5])\n",
        "b = np.array([0.1, 0.4, 0.5])\n",
        "\n",
        "# Calculate cross-entropy\n",
        "print(\"Cross-Entropy:\", cross_entropy(a, b))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWr1VcGuulB3",
        "outputId": "7424d754-9259-4127-c5f1-38d01fe491f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entropy: 1.4854752972273344\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def entropy(a):\n",
        "    return -np.sum(a * np.log2(a))\n",
        "\n",
        "# Example\n",
        "a = np.array([0.2, 0.3, 0.5])\n",
        "print(\"Entropy:\", entropy(a))\n",
        "\n",
        "# Example using scipy\n",
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "coin_toss = [0.5, 0.5] # Considering a fair coin\n",
        "entropy(coin_toss, base=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1dfS7j8A_3R",
        "outputId": "98cd52bd-8517-481d-9f1c-46b63ac6cc3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mutual Information: [1.01498115 1.07387267 1.03460123]\n",
            "0.5623351446188083\n"
          ]
        }
      ],
      "source": [
        "def mutual_info(p_joint, p_x, p_y):\n",
        "    return entropy(p_x) + entropy(p_y) - entropy(p_joint)\n",
        "\n",
        "# Example data\n",
        "p_joint = np.array([[0.1, 0.1, 0.2], [0.1, 0.2, 0.3], [0.1, 0.1, 0.2]])\n",
        "p_x = np.sum(p_joint, axis=1)\n",
        "p_y = np.sum(p_joint, axis=0)\n",
        "\n",
        "# Calculate mutual information\n",
        "print(\"Mutual Information:\", mutual_info( p_joint, p_x, p_y))\n",
        "\n",
        "# Example using library\n",
        "from sklearn.metrics import mutual_info_score\n",
        "a = ['A', 'B', 'A', 'C',  'B', 'A', 'B', 'C']\n",
        "x = ['X', 'X', 'Y', 'Z', 'Z', 'Y', 'Z', 'Z']\n",
        "mi = mutual_info_score(a,x)\n",
        "print(mi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjcNIcaqA_6m",
        "outputId": "03e35ec2-6e1e-43c5-bdc5-cfebcbbd8dcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conditional Entropy: [ 0.04369212 -0.0151994   0.02407204]\n"
          ]
        }
      ],
      "source": [
        "# Here p_joint is the joint entropy of two random variables X and Y\n",
        "def cond_entropy(p_joint, prob_y):\n",
        "    return entropy(p_joint) - entropy(prob_y) # According to the chain rule\n",
        "\n",
        "# Example data\n",
        "p_joint = np.array([[0.1, 0.1, 0.2], [0.1, 0.2, 0.3], [0.1, 0.1, 0.2]])\n",
        "prob_y = np.array([0.1, 0.2, 0.2])\n",
        "\n",
        "# Calculate conditional entropy\n",
        "print(\"Conditional Entropy:\",  cond_entropy(p_joint, prob_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUNYauOeLymR",
        "outputId": "3c4e874f-0600-4395-c54e-353f8e24865d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KL Divergence: 0.05232481437645474\n"
          ]
        }
      ],
      "source": [
        "def kl_divergence(p, q):\n",
        "    return np.sum(np.where(p != 0, p * np.log(p / q), 0))\n",
        "\n",
        "# Example \n",
        "a = np.array([0.2, 0.3, 0.5])\n",
        "b = np.array([0.1, 0.4, 0.5])\n",
        "\n",
        "# Calculate KL divergence\n",
        "print(\"KL Divergence:\",  kl_divergence(a, b))    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
